# -*- coding: utf-8 -*-
"""Insights Snippet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AKqKzzNW2rW1-5bcrVXgv6sKeLN2Unim
"""

import requests
import os
import pandas as pd

url = "https://api.rootfi.dev/v3/accounting/bill_payments"
url_1 = "https://api.rootfi.dev/v3/accounting/bill_credit_notes"
url_2 = "https://api.rootfi.dev/v3/accounting/Accounts"
url_a = "https://api.rootfi.dev/v3/accounting/Invoice_Payments"
url_b = "https://api.rootfi.dev/v3/accounting/Invoice_Credit_Notes"
url_c = "https://api.rootfi.dev/v3/accounting/Invoices"

os.environ["ROOTIFY_API"] = "19a42cfe-170d-416a-8dab-cc8f227ae0b0"
os.environ["COMPANY_ID"] = "13288"

def fetch_all_bill_payments(api_key, company_id, base_url, limit=1000):
    headers = {"api_key": api_key}
    all_bill_payments = []
    next_cursor = None

    while True:
        params = {
            "limit": limit,
            "rootfi_company_id[eq]": company_id
        }
        if next_cursor:
            params["next"] = next_cursor

        response = requests.get(base_url, headers=headers, params=params)
        response.raise_for_status()
        data = response.json()

        all_bill_payments.extend(data["data"])

        if not data.get("next"):
            break

        next_cursor = data["next"]

    return all_bill_payments

df = pd.json_normalize(fetch_all_bill_payments(os.environ["ROOTIFY_API"], os.environ["COMPANY_ID"],url))
df_2 = pd.json_normalize(fetch_all_bill_payments(os.environ["ROOTIFY_API"], os.environ["COMPANY_ID"],url_1))
df_3 = pd.json_normalize(fetch_all_bill_payments(os.environ["ROOTIFY_API"], os.environ["COMPANY_ID"],url_2))
df_a = pd.json_normalize(fetch_all_bill_payments(os.environ["ROOTIFY_API"], os.environ["COMPANY_ID"],url_a))
df_b = pd.json_normalize(fetch_all_bill_payments(os.environ["ROOTIFY_API"], os.environ["COMPANY_ID"],url_b))
df_c = pd.json_normalize(fetch_all_bill_payments(os.environ["ROOTIFY_API"], os.environ["COMPANY_ID"],url_c))

columns_to_drop = [
    'rootfi_id', 'rootfi_created_at', 'rootfi_updated_at', 'rootfi_deleted_at',
    'rootfi_company_id', 'platform_id', 'platform_unique_id', 'bill_id',
    'credit_note_id', 'updated_at', 'document_number', 'currency_rate',
    'custom_fields', 'currency_id', 'payment_mode'
]

# Drop the columns from the DataFrame
df = df.drop(columns=columns_to_drop)

rename_mapping = {
    'payment_id': 'Bill_Payment_ID',
    'account_id': 'ID',
    'contact_id': 'Vendor_Contact_ID',
    'amount': 'Payment_Amount',
    'memo': 'Payment_Memo',
    'payment_date': 'Bill_Payment_Date'
}

# Rename columns in the DataFrame
df = df.rename(columns=rename_mapping)

columns_to_drop_2 = [
    'rootfi_id', 'rootfi_created_at', 'rootfi_updated_at', 'rootfi_deleted_at',
    'rootfi_company_id', 'platform_id', 'platform_unique_id','currency_id',
    'total_discount' ,	'tax_amount', 'document_number',	'remaining_credit',
    'custom_fields',	'updated_at', 	'bill_ids'
]

# Drop the columns from the DataFrame
df_2 = df_2.drop(columns=columns_to_drop_2)

rename_mapping_2 = {
    'contact_id': 'Vendor_Contact_ID',
    'total_amount': 'Credit_Note_Total_Amount',
    'posted_date': 'Credit_Note_Posted_Date',
    'memo': 'Credit_Note_Memo',
    'status': 'Credit_Note_Status'
}

# Rename columns in the DataFrame
df_2 = df_2.rename(columns=rename_mapping_2)

columns_to_drop_a = [
    'rootfi_id', 'rootfi_created_at', 'rootfi_updated_at', 'rootfi_deleted_at',
    'rootfi_company_id', 'platform_id', 'platform_unique_id','invoice_id',
    'credit_note_id', 'updated_at', 'document_number', 'currency_rate',
    'custom_fields', 'currency_id', 'payment_mode'
]

# Drop the columns from the DataFrame
df_a = df_a.drop(columns=columns_to_drop_a)

rename_mapping = {
    'payment_id': 'Invoice_Payment_ID',
    'account_id': 'ID',
    'contact_id': 'Vendor_Contact_ID',
    'amount': 'Payment_Amount',
    'memo': 'Payment_Memo',
    'payment_date': 'Invoice_Payment_Date'
}

# Rename columns in the DataFrame
df_a = df_a.rename(columns=rename_mapping)

columns_to_drop_b = [
    'rootfi_id', 'rootfi_created_at', 'rootfi_updated_at', 'rootfi_deleted_at',
    'rootfi_company_id', 'platform_id', 'platform_unique_id','currency_id',
     'document_number', 'custom_fields',	'tax_amount', 	'total_discount', 'remaining_credit', 'status',
       'updated_at','invoice_ids'
]

# Drop the columns from the DataFrame
df_b = df_b.drop(columns=columns_to_drop_b)

rename_mapping_b = {
    'payment_id': 'Invoice_Payment_ID',
    'contact_id':'Vendor_Contact_ID',
    'total_amount' : 'Payment_Amount',
    'memo': 'Payment_Memo',
    'posted_date': 'Invoice_Payment_Date'
}

# Rename columns in the DataFrame
df_b = df_b.rename(columns=rename_mapping_b)

columns_to_drop_c = [
    'rootfi_id',	'rootfi_created_at',	'rootfi_updated_at',	'rootfi_deleted_at',
    'rootfi_company_id',	'platform_id',	'platform_unique_id',	'currency_id',		'custom_fields',
  	'updated_at', 'currency_rate', 'document_number', 'total_discount','sub_total', 'tax_amount', 'amount_due', 'due_date', 'status', 'currency_rate', 'sales_order_ids']

# Drop the columns from the DataFrame
df_c = df_c.drop(columns=columns_to_drop_c)

rename_mapping = {
    'contact_id': 'Vendor_Contact_ID',
    'total_amount': 'Payment_Amount',
    'memo': 'Payment_Memo',
    'posted_date': 'Invoice_Payment_Date'
}

# Rename columns in the DataFrame
df_c = df_c.rename(columns=rename_mapping)

columns_to_drop_3 = [
    'rootfi_id',	'rootfi_created_at',	'rootfi_updated_at',	'rootfi_deleted_at',	'rootfi_company_id',	'platform_id',	'platform_unique_id',	'currency_id',
    'parent_account_id',	'custom_fields',	'nominal_code',	'description',	'current_balance',	'updated_at',	'status'	,'category'
]

# Drop the columns from the DataFrame
df_3 = df_3.drop(columns=columns_to_drop_3)


rename_mapping = {
    'name': 'ID',
    'sub_category': 'Expense_Category'
}

# Rename columns in the DataFrame
df_3 = df_3.rename(columns=rename_mapping)

df = df.merge(
    df_3,
    on='ID',
    how='left'
)

!pip install protobuf==3.20.3
!pip install -qU  langchain-core
!pip install -qU langchain-community
!pip install -qU  'crewai[tools]'
!pip install -qU langchain-groq

import json
import os
import sqlite3
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from pathlib import Path
from textwrap import dedent
from typing import Any, Dict, List, Tuple, Union

import pandas as pd
from crewai import Agent, Crew, Process, Task , LLM
from crewai_tools import tools
from google.colab import userdata
from langchain.schema import AgentFinish
from langchain.schema.output import LLMResult
from langchain_community.tools.sql_database.tool import (
    InfoSQLDatabaseTool,
    ListSQLDatabaseTool,
    QuerySQLCheckerTool,
    QuerySQLDataBaseTool,
)
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq

from google.colab import userdata
# os.environ["GROQ_API_KEY"] = "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"
os.environ["GROQ_API_KEY"] = "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"

create_table_query_1 = """
CREATE TABLE IF NOT EXISTS Bill_Payments (
    Bill_Payment_ID VARCHAR(255) ,
    ID VARCHAR(255),
    Vendor_Contact_ID VARCHAR(255),
    Payment_Amount DECIMAL(15, 2),
    Payment_Memo TEXT,
    Bill_Payment_Date VARCHAR(50),
    Expense_Category VARCHAR(255)
);
"""

create_table_query_2 = """
CREATE TABLE IF NOT EXISTS Bill_Credit_Notes (
    Vendor_Contact_ID VARCHAR(255),
    Credit_Note_Total_Amount DECIMAL(15, 2),
    Credit_Note_Posted_Date VARCHAR(50),
    Credit_Note_Memo TEXT,
    Credit_Note_Status VARCHAR(50)
);

"""

create_table_query_c = """
CREATE TABLE IF NOT EXISTS Invoices (
    Vendor_Contact_ID VARCHAR(255),
    Payment_Amount DECIMAL(15, 2),
    Payment_Memo TEXT,
    Invoice_Payment_Date VARCHAR(50)
);
"""

create_table_query_a = """
CREATE TABLE IF NOT EXISTS Invoice_Payments (
    Invoice_Payment_ID VARCHAR(255) ,
    ID VARCHAR(255),
    Vendor_Contact_ID VARCHAR(255),
    Payment_Amount DECIMAL(15, 2),
    Payment_Memo TEXT,
    Invoice_Payment_Date VARCHAR(50)
);
"""

create_table_query_b = """
CREATE TABLE IF NOT EXISTS Invoice_Credit_Notes (
    Vendor_Contact_ID VARCHAR(255),
    Payment_Amount DECIMAL(15, 2),
    Payment_Memo TEXT,
    Invoice_Payment_Date VARCHAR(50)
);
"""



conn = sqlite3.connect("Bills.db")
cursor = conn.cursor()
cursor.execute(create_table_query_1)
cursor.execute(create_table_query_2)
cursor.execute(create_table_query_a)
cursor.execute(create_table_query_b)
cursor.execute(create_table_query_c)

cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
cursor.fetchall()

# Step 5: Insert Data from DataFrame to SQLite Table
# Using 'if_exists="replace"' to overwrite any existing table
df.to_sql("Bill_Payments", conn, if_exists="append", index=False)
df_2.to_sql("Bill_Credit_Notes", conn, if_exists="append", index=False)
df_a.to_sql("Invoice_Payments", conn, if_exists="append", index=False)
df_b.to_sql("Invoice_Credit_Notes", conn, if_exists="append", index=False)
df_c.to_sql("Invoices", conn, if_exists="append", index=False)

# Commit the changes and close the connection
conn.commit()
conn.close()

print("Data successfully inserted into the Bill DB.")



from langchain_groq import ChatGroq

llm = ChatGroq(
    model="llama3-groq-70b-8192-tool-use-preview",
    temperature=0.5,
    max_tokens=1024,
)

from langchain_openai import ChatOpenAI

)

from langchain.agents import (
    AgentExecutor,
    create_react_agent,
)
from langchain_core.tools import Tool

data_base = SQLDatabase.from_uri("sqlite:///Bills.db")

# @tool("get_average_cash_flows")
def get_average_cash_flows(inputs: str) -> str:
    """
    Calculate average cash inflows and outflows for a selected duration.
    Accepts input as a JSON-like string containing 'start_date' and 'end_date'.
    """
    import json
    try:
        # Parse the input string into a dictionary
        input_dict = json.loads(inputs)
        start_date = input_dict["start_date"]
        end_date = input_dict["end_date"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    inflow_query = """
        SELECT AVG(Payment_Amount) as avg_inflow
        FROM Invoice_Payments
        WHERE Invoice_Payment_Date BETWEEN ? AND ?
    """

    outflow_query = """
        SELECT AVG(Payment_Amount) as avg_outflow
        FROM Bill_Payments
        WHERE Bill_Payment_Date BETWEEN ? AND ?
    """

    cursor.execute(inflow_query, (start_date, end_date))
    avg_inflow = cursor.fetchone()[0] or 0

    cursor.execute(outflow_query, (start_date, end_date))
    avg_outflow = cursor.fetchone()[0] or 0

    conn.close()

    return f"Average Inflow: {avg_inflow:.2f}, Average Outflow: {avg_outflow:.2f}"

# @tool("analyze_monthly_trends")
def analyze_monthly_trends(year: str) -> str:
    """
    Analyze monthly cash flow trends for a specific year.
    Args:
        year (str): Year to analyze
    Returns:
        String containing monthly trend analysis
    """
    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    monthly_query = """
        WITH monthly_inflows AS (
    SELECT
        strftime('%Y-%m', Invoice_Payment_Date) as month,
        SUM(Payment_Amount) as total_inflow
    FROM Invoice_Payments
    WHERE strftime('%Y', Invoice_Payment_Date) = ?
    GROUP BY month
),
monthly_outflows AS (
    SELECT
        strftime('%Y-%m', Bill_Payment_Date) as month,
        SUM(Payment_Amount) as total_outflow
    FROM Bill_Payments
    WHERE strftime('%Y', Bill_Payment_Date) = ?
    GROUP BY month
),
combined_data AS (
    SELECT
        mi.month as month,
        mi.total_inflow as total_inflow,
        mo.total_outflow as total_outflow
    FROM monthly_inflows mi
    LEFT JOIN monthly_outflows mo ON mi.month = mo.month
    UNION
    SELECT
        mo.month as month,
        mi.total_inflow as total_inflow,
        mo.total_outflow as total_outflow
    FROM monthly_outflows mo
    LEFT JOIN monthly_inflows mi ON mo.month = mi.month
)
SELECT
    month,
    COALESCE(total_inflow, 0) as total_inflow,
    COALESCE(total_outflow, 0) as total_outflow,
    COALESCE(total_inflow, 0) - COALESCE(total_outflow, 0) as net_flow
FROM combined_data
ORDER BY month;
    """

    cursor.execute(monthly_query, (year, year))
    results = cursor.fetchall()
    conn.close()

    if not results:
        return f"No data found for year {year}"

    analysis = f"Monthly Cash Flow Analysis for {year}:\n"
    for row in results:
        analysis += f"\nMonth {row[0]}:"
        analysis += f"\n  Inflow: {row[1]:.2f}"
        analysis += f"\n  Outflow: {row[2]:.2f}"
        analysis += f"\n  Net Flow: {row[3]:.2f}"

    return analysis

# @tool("analyze_revenue_streams")
def analyze_revenue_streams(inputs: str) -> str:
    """
    Analyze revenue streams and their reliability.
    Accepts input as a JSON-like string containing 'period_start' and 'period_end'.
    Args:
        period_start (str): Start date in YYYY-MM-DD format
        period_end (str): End date in YYYY-MM-DD format
    Returns:
        String containing revenue stream analysis
    """
    import json
    try:
        # Parse the input string into a dictionary
        input_dict = json.loads(inputs)
        period_start = input_dict["period_start"]
        period_end = input_dict["period_end"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    revenue_query = """
        WITH revenue_data AS (
            SELECT
                Vendor_Contact_ID,
                SUM(Payment_Amount) as total_amount,
                COUNT(*) as transaction_count,
                AVG(Payment_Amount) as avg_payment
            FROM Invoice_Payments
            WHERE Invoice_Payment_Date BETWEEN ? AND ?
            GROUP BY Vendor_Contact_ID
        )
        SELECT
            Vendor_Contact_ID,
            total_amount,
            transaction_count,
            avg_payment,
            (total_amount * 1.0 / (SELECT SUM(total_amount) FROM revenue_data) * 100) as percentage
        FROM revenue_data
        ORDER BY total_amount DESC
        LIMIT 10
    """

    cursor.execute(revenue_query, (period_start, period_end))
    results = cursor.fetchall()
    conn.close()

    analysis = "Top Revenue Streams Analysis:\n"
    for row in results:
        analysis += f"\nVendor: {row[0]}"
        analysis += f"\n  Total Amount: {row[1]:.2f}"
        analysis += f"\n  Transaction Count: {row[2]}"
        analysis += f"\n  Average Payment: {row[3]:.2f}"
        analysis += f"\n  Revenue Percentage: {row[4]:.2f}%"

    return analysis

# @tool("analyze_expenses")
def analyze_expenses(inputs: str) -> str:
    """
    Get comprehensive expense analysis including categories and patterns.
    Accepts input as a JSON-like string containing 'period_start' and 'period_end'.
    Args:
        period_start (str): Start date in YYYY-MM-DD format
        period_end (str): End date in YYYY-MM-DD format
    Returns:
        String containing detailed expense analysis
    """
    import json
    try:
        # Parse the input string into a dictionary
        input_dict = json.loads(inputs)
        period_start = input_dict["period_start"]
        period_end = input_dict["period_end"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    query = """
        WITH expense_analysis AS (
            SELECT
                COALESCE(Expense_Category, 'Uncategorized') as category,
                COUNT(*) as transaction_count,
                SUM(Payment_Amount) as total_amount,
                AVG(Payment_Amount) as avg_amount,
                MIN(Payment_Amount) as min_amount,
                MAX(Payment_Amount) as max_amount
            FROM Bill_Payments
            WHERE Bill_Payment_Date BETWEEN ? AND ?
            GROUP BY Expense_Category
        )
        SELECT
            category,
            transaction_count,
            total_amount,
            avg_amount,
            min_amount,
            max_amount,
            (total_amount / (SELECT SUM(total_amount) FROM expense_analysis) * 100) as percentage
        FROM expense_analysis
        ORDER BY total_amount DESC
    """

    cursor.execute(query, (period_start, period_end))
    results = cursor.fetchall()
    conn.close()

    analysis = "Comprehensive Expense Analysis:\n"
    total_expenses = sum(row[2] for row in results)

    for row in results:
        analysis += f"\nCategory: {row[0]}"
        analysis += f"\n  Transaction Count: {row[1]}"
        analysis += f"\n  Total Amount: {row[2]:.2f}"
        analysis += f"\n  Average Amount: {row[3]:.2f}"
        analysis += f"\n  Range: {row[4]:.2f} to {row[5]:.2f}"
        analysis += f"\n  Percentage of Total: {row[6]:.2f}%"

    return analysis

#q5
def analyze_receivables_collection(inputs: str) -> str:
    """
    Analyze receivables collection patterns and their impact on cash flow.
    Args:
        period_start (str): Start date in YYYY-MM-DD format
        period_end (str): End date in YYYY-MM-DD format
    Returns:
        String containing receivables analysis
    """
    import json
    try:
        # Parse the input string into a dictionary
        input_dict = json.loads(inputs)
        period_start = input_dict["period_start"]
        period_end = input_dict["period_end"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    try:
        # Get overall collection statistics
        collection_query = """
        SELECT
            COUNT(DISTINCT Vendor_Contact_ID) as unique_customers,
            COUNT(*) as total_payments,
            SUM(Payment_Amount) as total_collected,
            AVG(Payment_Amount) as avg_payment
        FROM Invoice_Payments
        WHERE Invoice_Payment_Date BETWEEN ? AND ?
        """

        cursor.execute(collection_query, (period_start, period_end))
        overall_stats = cursor.fetchone()

        # Get customer-wise collection patterns
        customer_query = """
        SELECT
            Vendor_Contact_ID,
            COUNT(*) as payment_count,
            SUM(Payment_Amount) as total_amount,
            MIN(Invoice_Payment_Date) as first_payment,
            MAX(Invoice_Payment_Date) as last_payment,
            AVG(Payment_Amount) as avg_amount
        FROM Invoice_Payments
        WHERE Invoice_Payment_Date BETWEEN ? AND ?
        GROUP BY Vendor_Contact_ID
        ORDER BY total_amount DESC
        LIMIT 5
        """

        cursor.execute(customer_query, (period_start, period_end))
        customer_patterns = cursor.fetchall()

        # Format analysis
        analysis = "Receivables Collection Analysis:\n\n"

        if overall_stats[0]:  # if we have data
            analysis += f"Overall Collection Statistics ({period_start} to {period_end}):\n"
            analysis += f"• Number of Unique Customers: {overall_stats[0]}\n"
            analysis += f"• Total Payments Received: {overall_stats[1]}\n"
            analysis += f"• Total Amount Collected: {overall_stats[2]:,.2f}\n"
            analysis += f"• Average Payment Amount: {overall_stats[3]:,.2f}\n"

            # Calculate daily collection rate
            days_query = """
            SELECT COUNT(DISTINCT date(Invoice_Payment_Date))
            FROM Invoice_Payments
            WHERE Invoice_Payment_Date BETWEEN ? AND ?
            """
            cursor.execute(days_query, (period_start, period_end))
            active_days = cursor.fetchone()[0]

            daily_collection = overall_stats[2] / active_days if active_days > 0 else 0
            analysis += f"• Average Daily Collection: {daily_collection:,.2f}\n"

            # Top customer analysis
            analysis += "\nTop 5 Customers by Collection Amount:\n"
            for customer in customer_patterns:
                analysis += f"\nCustomer: {customer[0]}"
                analysis += f"\n• Number of Payments: {customer[1]}"
                analysis += f"\n• Total Amount: {customer[2]:,.2f}"
                analysis += f"\n• Average Payment: {customer[5]:,.2f}"
                analysis += f"\n• First Payment: {customer[3]}"
                analysis += f"\n• Last Payment: {customer[4]}"

            # Cash Flow Impact Analysis
            analysis += "\n\nCash Flow Impact:\n"
            total_amount = overall_stats[2]
            if total_amount > 0:
                analysis += f"• Daily Cash Inflow Rate: {daily_collection:,.2f}\n"

                # Calculate collection concentration
                top_customer_amount = customer_patterns[0][2] if customer_patterns else 0
                concentration = (top_customer_amount / total_amount * 100) if total_amount > 0 else 0
                analysis += f"• Collection Concentration: {concentration:.1f}% from top customer\n"

        else:
            analysis += "No collection data found for the specified period."

    except Exception as e:
        analysis = f"Error analyzing receivables collection: {str(e)}"
    finally:
        conn.close()

    return analysis
#q7
def analyze_vendor_payments(inputs: str) -> str:
    """
    Analyze average time to pay vendors and its impact on cash flow health.
    Accepts input as a JSON-like string containing 'period_start' and 'period_end'.
    Args:
        inputs (str): JSON string containing 'period_start' and 'period_end'
    Returns:
        String containing vendor payment analysis
    """
    import json
    import sqlite3

    try:
        # Parse the input string into a dictionary
        input_dict = json.loads(inputs)
        period_start = input_dict["period_start"]
        period_end = input_dict["period_end"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    # Query to calculate vendor payment patterns
    vendor_query = """
        SELECT
            Vendor_Contact_ID,
            AVG(JULIANDAY(Bill_Payment_Date) - JULIANDAY(?)) AS avg_payment_time,
            SUM(Payment_Amount) AS total_paid,
            COUNT(*) AS payment_count
        FROM Bill_Payments
        WHERE Bill_Payment_Date BETWEEN ? AND ?
        GROUP BY Vendor_Contact_ID
        ORDER BY avg_payment_time DESC
    """

    cursor.execute(vendor_query, (period_start, period_start, period_end))
    vendor_results = cursor.fetchall()

    # Query to calculate cash flow impact
    cash_flow_query = """
        SELECT
            SUM(Payment_Amount) AS total_outflow,
            COUNT(DISTINCT Vendor_Contact_ID) AS vendor_count
        FROM Bill_Payments
        WHERE Bill_Payment_Date BETWEEN ? AND ?
    """

    cursor.execute(cash_flow_query, (period_start, period_end))
    cash_flow_results = cursor.fetchone()

    # Include credit notes impact from Bill_Credit_Notes
    credit_note_query = """
        SELECT
            SUM(Credit_Note_Total_Amount) AS total_credit_notes,
            COUNT(*) AS credit_note_count
        FROM Bill_Credit_Notes
        WHERE Credit_Note_Posted_Date BETWEEN ? AND ?
    """

    cursor.execute(credit_note_query, (period_start, period_end))
    credit_note_results = cursor.fetchone()

    conn.close()

    total_credit_notes = credit_note_results[0] if credit_note_results[0] is not None else 0.0
    credit_note_count = credit_note_results[1] if credit_note_results[1] is not None else 0

    # Prepare the analysis output
    analysis = f"""Vendor Payment Analysis:
    Total Vendors: {cash_flow_results[1]}
    Total Outflow (Payments): {cash_flow_results[0]:.2f}
    Total Credit Notes Applied: {total_credit_notes:.2f} (from {credit_note_count} credit notes)

    Vendor Payment Patterns:"""

    for row in vendor_results[:10]:  # Limit to the top 10 vendors
        analysis += f"""
        Vendor: {row[0]}
        Average Payment Time: {row[1]:.2f} days (from {period_start})
        Total Paid: {row[2]:.2f}
        Payment Count: {row[3]}
        """

    return analysis


#q8 What were our top 10 largest cash outflow transactions last year, and what were they for?
def analyze_large_outflows(year: str) -> str:
    """
    Identify and analyze top 10 largest cash outflow transactions.
    Args:
        year (str): Year to analyze in YYYY format
    Returns:
        String containing large outflow analysis
    """
    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    outflow_query = """
        SELECT
            Bill_Payment_Date,
            Vendor_Contact_ID,
            Payment_Amount,
            Payment_Memo,
            Expense_Category
        FROM Bill_Payments
        WHERE strftime('%Y', Bill_Payment_Date) = ?
        ORDER BY Payment_Amount DESC
        LIMIT 10
    """

    cursor.execute(outflow_query, (year,))
    results = cursor.fetchall()
    conn.close()

    analysis = f"Top 10 Largest Cash Outflows for {year}:\n"
    for row in results:
        analysis += f"""
        Date: {row[0]}
        Vendor: {row[1]}
        Amount: {row[2]:.2f}
        Purpose: {row[3]}
        Category: {row[4] or 'Uncategorized'}
        """

    return analysis

#q9  What is our cash conversion cycle, and are there areas where we can reduce the time it takes to turn cash invested in operations back into cash inflows?

def analyze_cash_conversion_cycle(inputs: str) -> str:
    """
    Analyze cash conversion cycle and identify areas for improvement.
    Args:
        period_start (str): Start date in YYYY-MM-DD format
        period_end (str): End date in YYYY-MM-DD format
    Returns:
        String containing cash conversion cycle analysis
    """
    import json
    try:
        # Parse the input string into a dictionary
        input_dict = json.loads(inputs)
        period_start = input_dict["period_start"]
        period_end = input_dict["period_end"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()
    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    try:
        # Analyze Invoice Payment Patterns (Cash Inflows)
        inflow_query = """
        SELECT
            COUNT(*) as total_transactions,
            SUM(Payment_Amount) as total_amount,
            AVG(Payment_Amount) as avg_amount,
            MIN(Invoice_Payment_Date) as earliest_date,
            MAX(Invoice_Payment_Date) as latest_date
        FROM Invoice_Payments
        WHERE Invoice_Payment_Date BETWEEN ? AND ?
        """

        # Analyze Bill Payment Patterns (Cash Outflows)
        outflow_query = """
        SELECT
            COUNT(*) as total_transactions,
            SUM(Payment_Amount) as total_amount,
            AVG(Payment_Amount) as avg_amount,
            MIN(Bill_Payment_Date) as earliest_date,
            MAX(Bill_Payment_Date) as latest_date
        FROM Bill_Payments
        WHERE Bill_Payment_Date BETWEEN ? AND ?
        """

        cursor.execute(inflow_query, (period_start, period_end))
        inflow_stats = cursor.fetchone()

        cursor.execute(outflow_query, (period_start, period_end))
        outflow_stats = cursor.fetchone()

        analysis = "Cash Conversion Cycle Analysis:\n\n"

        if inflow_stats and outflow_stats:
            # Calculate key metrics
            inflow_days = len(set(row[0] for row in cursor.execute(
                "SELECT Invoice_Payment_Date FROM Invoice_Payments WHERE Invoice_Payment_Date BETWEEN ? AND ?",
                (period_start, period_end)
            ).fetchall()))

            outflow_days = len(set(row[0] for row in cursor.execute(
                "SELECT Bill_Payment_Date FROM Bill_Payments WHERE Bill_Payment_Date BETWEEN ? AND ?",
                (period_start, period_end)
            ).fetchall()))

            # Cash Flow Patterns
            analysis += "Cash Flow Patterns:\n"
            analysis += f"1. Cash Inflows (Collections):\n"
            analysis += f"   • Total Collections: {inflow_stats[1]:,.2f}\n"
            analysis += f"   • Number of Collection Days: {inflow_days}\n"
            analysis += f"   • Average Daily Collection: {(inflow_stats[1]/inflow_days if inflow_days else 0):,.2f}\n\n"

            analysis += f"2. Cash Outflows (Payments):\n"
            analysis += f"   • Total Payments: {outflow_stats[1]:,.2f}\n"
            analysis += f"   • Number of Payment Days: {outflow_days}\n"
            analysis += f"   • Average Daily Payment: {(outflow_stats[1]/outflow_days if outflow_days else 0):,.2f}\n\n"

            # Net Cash Flow
            net_cash_flow = inflow_stats[1] - outflow_stats[1]
            analysis += f"3. Net Cash Position:\n"
            analysis += f"   • Net Cash Flow: {net_cash_flow:,.2f}\n"

            # Optimization Recommendations
            analysis += "\nOptimization Opportunities:\n"

            # Collection Improvements
            if inflow_stats[1] > 0:
                avg_transaction_size = inflow_stats[1] / inflow_stats[0]
                analysis += "1. Collection Process:\n"
                if avg_transaction_size > 100000:
                    analysis += "   • Consider implementing progressive billing for large transactions\n"
                analysis += "   • Implement early payment incentives for faster collections\n"

            # Payment Optimization
            if outflow_stats[1] > 0:
                analysis += "\n2. Payment Process:\n"
                analysis += "   • Review payment scheduling to optimize cash retention\n"
                analysis += "   • Consider vendor payment terms negotiation\n"

            # Cash Flow Timing
            analysis += "\n3. Cash Flow Timing:\n"
            if net_cash_flow < 0:
                analysis += "   • Priority: Accelerate collections to improve cash position\n"
            else:
                analysis += "   • Maintain current collection efficiency\n"

            analysis += "   • Schedule major payments to align with expected collections\n"

        else:
            analysis += "Insufficient data for cash conversion cycle analysis."

    except Exception as e:
        analysis = f"Error analyzing cash conversion cycle: {str(e)}"
    finally:
        conn.close()

    return analysis

#q6
#q6
# @tool("analyze_customer_delays")
def analyze_customer_delays(inputs: str) -> str:
    """
    Identify customers with longest payment delays and their outstanding amounts.
    Args:
        period_start (str): Start date in YYYY-MM-DD format
        period_end (str): End date in YYYY-MM-DD format
    Returns:
        String containing customer payment delay analysis
    """
    import json
    try:
        # Parse the input string into a dictionary
        input_dict = json.loads(inputs)
        period_start = input_dict["period_start"]
        period_end = input_dict["period_end"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    # Query to analyze payment patterns by customer
    delay_query = """
    WITH CustomerPayments AS (
        SELECT
            Vendor_Contact_ID,
            Invoice_Payment_Date,
            Payment_Amount,
            STRFTIME('%Y-%m-%d', Invoice_Payment_Date) as payment_date
        FROM Invoice_Payments
        WHERE Invoice_Payment_Date BETWEEN ? AND ?
    ),
    CustomerStats AS (
        SELECT
            Vendor_Contact_ID,
            COUNT(*) as payment_count,
            SUM(Payment_Amount) as total_amount,
            MIN(payment_date) as first_payment,
            MAX(payment_date) as last_payment,
            AVG(Payment_Amount) as avg_payment
        FROM CustomerPayments
        GROUP BY Vendor_Contact_ID
        HAVING payment_count > 1
    )
    SELECT
        cs.Vendor_Contact_ID,
        cs.payment_count,
        cs.total_amount,
        cs.first_payment,
        cs.last_payment,
        cs.avg_payment,
        JULIANDAY(cs.last_payment) - JULIANDAY(cs.first_payment) as date_range
    FROM CustomerStats cs
    ORDER BY total_amount DESC
    LIMIT 10
    """

    try:
        cursor.execute(delay_query, (period_start, period_end))
        results = cursor.fetchall()

        if not results:
            return "No payment data found for the specified period."

        analysis = "Top 10 Customers by Payment Analysis:\n\n"

        for row in results:
            vendor_id, payment_count, total_amount, first_payment, last_payment, avg_payment, date_range = row

            # Calculate average days between payments
            avg_days_between = date_range / payment_count if payment_count > 0 else 0

            analysis += f"""Customer: {vendor_id}
- Total Payments: {payment_count}
- Total Amount: {total_amount:.2f}
- Average Payment: {avg_payment:.2f}
- First Payment: {first_payment}
- Last Payment: {last_payment}
- Average Days Between Payments: {avg_days_between:.1f}

"""

        # Add summary statistics
        total_volume = sum(row[2] for row in results)
        avg_payment_size = sum(row[5] for row in results) / len(results)

        analysis += f"\nSummary Statistics:"
        analysis += f"\n• Total Payment Volume: {total_volume:.2f}"
        analysis += f"\n• Average Payment Size: {avg_payment_size:.2f}"

    except Exception as e:
        analysis = f"Error analyzing customer delays: {str(e)}"
    finally:
        conn.close()

    return analysis

tools = [
    Tool(
        name="get_average_cash_flows",
        func= get_average_cash_flows,
        description="Calculate average cash inflows and outflows for a selected duration. Requires a dictionary input with keys 'start_date' and 'end_date'.",
    ),
    Tool(
        name="analyze_monthly_trends",
        func=analyze_monthly_trends,
        description="Analyze monthly cash flow trends for a specific year. Requires a year as input.",
    ),
    Tool(
        name="analyze_revenue_streams",
        func=analyze_revenue_streams,
        description="Analyze revenue streams and their reliability. Requires a dictionary input with keys 'period_start' and 'period_end'.",
    ),
    Tool(
        name="analyze_expenses",
        func=analyze_expenses,
        description="Get comprehensive expense analysis including categories and patterns.Requires a dictionary input with keys 'period_start' and 'period_end'.",
    ),
    Tool(
        name="Analyze Receivables Collection",  # Tool name
        func=analyze_receivables_collection,  # Function that the tool will execute
        description="Analyze accounts receivable collection time and its impact on cash flow. Requires a dictionary input with keys 'period_start' and 'period_end'.",  # Description of the tool
    ),
    Tool(
        name="Analyze Delayed Payments",  # Tool name
        func=analyze_customer_delays,  # Function that the tool will execute
        description="Identify customers with significant payment delays and outstanding amounts",  # Description of the tool
    ),
    Tool(
        name="Analyze Vendor Payment Time",  # Tool name
        func=analyze_vendor_payments,  # Function that the tool will execute
        description="Analyze vendor payment timing and its impact on cash flow. Requires a dictionary input with keys 'period_start' and 'period_end'.",  # Description of the tool
    ),
    Tool(
        name="Get Largest Outflows",  # Tool name
        func=analyze_large_outflows,  # Function that the tool will execute
        description="Identify the largest cash outflow transactions for a specific year",  # Description of the tool
    ),
    Tool(
        name="Analyze Cash Conversion Cycle",  # Tool name
        func=analyze_cash_conversion_cycle,  # Function that the tool will execute
        description="Calculate and analyze the cash conversion cycle. Requires a dictionary input with keys 'period_start' and 'period_end'.",  # Description of the tool
    )
    # Tool(
    #     name="Analyze Fixed Variable Expenses",  # Tool name
    #     func=analyze_fixed_variable_expenses,  # Function that the tool will execute
    #     description="Analyze the proportion of fixed versus variable expenses. Requires a dictionary input with keys 'period_start' and 'period_end'.",  # Description of the tool
    # ),
]

template = """ Answer the following questions as best you can. You have access to the following tools:

{tools}

You are a **cash flow analyst agent** with expertise in analyzing financial data, trends, and expenses. Your goal is to interpret user queries, select the appropriate tool, and provide accurate insights.

### Tools Available:
1. **`get_average_cash_flows`**: Calculate average inflows and outflows for a date range.
2. **`analyze_monthly_trends`**: Analyze cash flow trends for a specific year.
3. **`analyze_revenue_streams`**: Analyze top revenue streams for a date range.
4. **`analyze_expenses`**: Perform detailed expense analysis for a date range.
5. **`list_tables`**: Retrieve available database tables.
6. **`tables_schema`**: Get schema and sample rows for tables.
7. **`execute_sql`**: Execute a SQL query and return results.
8. **`check_sql`**: Validate a SQL query before execution.

### Guidelines:
- Use the right tool based on the query.
- If inputs (e.g., dates) are missing, ask the user.
- Always validate SQL queries with `check_sql` before execution.
- Respond in a clear and professional manner.

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do.Convert the action input in suitable format matching with the function's Input
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {input}
Thought:{agent_scratchpad}  """

from langchain_core.prompts import PromptTemplate
prompt = PromptTemplate.from_template(template)

agent = create_react_agent(
    llm=llm,
    tools=tools,
    prompt=prompt,
    stop_sequence=True,
)

agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True,
    )

#q1
response = agent_executor.invoke({"input": "What is the average cash inflow and outflow for the selected duration  of year 2023?"})

#q2
response = agent_executor.invoke({"input": "Which months had the highest and lowest cash flow for year 2023, and what caused these fluctuations?"})

#q3
response = agent_executor.invoke({"input": "What percentage of our total cash inflows came from each revenue stream for year Jan 2023? Whiich are the most reliable revenie streams?"})

#q4
response = agent_executor.invoke({"input": "What percentage of our total cash outflows went to each major expense category in MAY 2023? "})

#q5
response = agent_executor.invoke({"input": "What is the average time it takes to collect receivables in  May 1 to May 31 of year 2023, and how does this impact cash flow?"})

#q6
response = agent_executor.invoke({"input": "Which customers have the longest payment delays in May, and how much is outstanding from them?"})

#q7
response = agent_executor.invoke({"input": "What is the average time to pay our vendors, and does this affect our cash flow health in the Month of May?"})

#q8
response = agent_executor.invoke({"input": "What were our top 10 largest cash outflow transactions for year 2023, and what were they for?"})

#q9
response = agent_executor.invoke({"input": "What is our cash conversion cycle for Jan 2023, and are there areas where we can reduce the time it takes to turn cash invested in operations back into cash inflows?"})



"""My Approach"""

# import sqlite3
# import json
# import re
# from datetime import datetime
# import calendar
# from langchain.agents import initialize_agent, Tool, AgentType
# from langchain.llms import OpenAI
# from langchain.agents import AgentExecutor

# # Database connection setup
# db_path = "Bills.db"

# def get_connection():
#     return sqlite3.connect(db_path)

# # Function to get average cash flows
# def get_average_cash_flows(inputs: str) -> str:
#     try:
#         input_dict = json.loads(inputs)
#         start_date = input_dict["start_date"]
#         end_date = input_dict["end_date"]
#     except (ValueError, KeyError) as e:
#         return f"Invalid input format: {e}"

#     conn = sqlite3.connect(db_path)
#     cursor = conn.cursor()

#     inflow_query = """
#         SELECT AVG(Payment_Amount) as avg_inflow
#         FROM Invoice_Payments
#         WHERE Invoice_Payment_Date BETWEEN ? AND ?;
#     """

#     outflow_query = """
#         SELECT AVG(Payment_Amount) as avg_outflow
#         FROM Bill_Payments
#         WHERE Bill_Payment_Date BETWEEN ? AND ?;
#     """

#     cursor.execute(inflow_query, (start_date, end_date))
#     avg_inflow = cursor.fetchone()[0] or 0

#     cursor.execute(outflow_query, (start_date, end_date))
#     avg_outflow = cursor.fetchone()[0] or 0

#     conn.close()

#     return f"Average Inflow: {avg_inflow:.2f}, Average Outflow: {avg_outflow:.2f}"


# # @tool("analyze_monthly_trends")
# def analyze_monthly_trends(year: str) -> str:
#     """
#     Analyze monthly cash flow trends for a specific year.
#     Args:
#         year (str): Year to analyze
#     Returns:
#         String containing monthly trend analysis
#     """
#     conn = sqlite3.connect("Bills.db")
#     cursor = conn.cursor()

#     monthly_query = """
#         WITH monthly_inflows AS (
#             SELECT
#                 strftime('%Y-%m', Invoice_Payment_Date) as month,
#                 SUM(Payment_Amount) as total_inflow
#             FROM Invoice_Payments
#             WHERE strftime('%Y', Invoice_Payment_Date) = ?
#             GROUP BY month
#         ),
#         monthly_outflows AS (
#             SELECT
#                 strftime('%Y-%m', Bill_Payment_Date) as month,
#                 SUM(Payment_Amount) as total_outflow
#             FROM Bill_Payments
#             WHERE strftime('%Y', Bill_Payment_Date) = ?
#             GROUP BY month
#         ),
#         combined_data AS (
#             SELECT
#                 mi.month as month,
#                 mi.total_inflow as total_inflow,
#                 mo.total_outflow as total_outflow
#             FROM monthly_inflows mi
#             LEFT JOIN monthly_outflows mo ON mi.month = mo.month
#             UNION
#             SELECT
#                 mo.month as month,
#                 mi.total_inflow as total_inflow,
#                 mo.total_outflow as total_outflow
#             FROM monthly_outflows mo
#             LEFT JOIN monthly_inflows mi ON mo.month = mi.month
#         )
#         SELECT
#             month,
#             COALESCE(total_inflow, 0) as total_inflow,
#             COALESCE(total_outflow, 0) as total_outflow,
#             COALESCE(total_inflow, 0) - COALESCE(total_outflow, 0) as net_flow
#         FROM combined_data
#         ORDER BY month;
#     """

#     cursor.execute(monthly_query, (year, year))
#     results = cursor.fetchall()
#     conn.close()

#     if not results:
#         return f"No data found for year {year}"

#     # Finding months with highest and lowest net cash flow
#     max_month = min_month = results[0]
#     for row in results:
#         if row[3] > max_month[3]: max_month = row
#         if row[3] < min_month[3]: min_month = row

#     analysis = f"Monthly Cash Flow Analysis for {year}:\n"
#     for row in results:
#         analysis += f"\nMonth {row[0]}:"
#         analysis += f"\n  Inflow: {row[1]:.2f}"
#         analysis += f"\n  Outflow: {row[2]:.2f}"
#         analysis += f"\n  Net Flow: {row[3]:.2f}"

#     analysis += f"\n\nThe month with the highest net cash flow is {max_month[0]} with a net flow of {max_month[3]:.2f}."
#     analysis += f"\nThe month with the lowest net cash flow is {min_month[0]} with a net flow of {min_month[3]:.2f}."
#     # For detailed insights about fluctuations, further query logic can be added.

#     return analysis

# # Function to calculate the percentage of inflows from each revenue stream
# def get_revenue_stream_percentages(inputs: str) -> str:
#     """
#     Calculate the percentage of cash inflows from each revenue stream for a given date range.
#     Args:
#         inputs (str): JSON-like string containing 'start_date' and 'end_date'.
#     Returns:
#         String summarizing revenue stream percentages.
#     """
#     import json
#     try:
#         # Parse inputs
#         input_dict = json.loads(inputs)
#         start_date = input_dict["start_date"]
#         end_date = input_dict["end_date"]
#     except (ValueError, KeyError) as e:
#         return f"Invalid input format: {e}"

#     conn = sqlite3.connect("Bills.db")
#     cursor = conn.cursor()

#     revenue_query = """
#         WITH revenue_streams AS (
#             SELECT
#                 Vendor_Contact_ID AS revenue_stream,
#                 SUM(Payment_Amount) AS total_inflow
#             FROM Invoice_Payments
#             WHERE Invoice_Payment_Date BETWEEN ? AND ?
#             GROUP BY Vendor_Contact_ID
#         )
#         SELECT
#             revenue_stream,
#             total_inflow,
#             (total_inflow * 1.0 / (SELECT SUM(total_inflow) FROM revenue_streams) * 100) AS percentage
#         FROM revenue_streams
#         ORDER BY total_inflow DESC
#     """

#     cursor.execute(revenue_query, (start_date, end_date))
#     results = cursor.fetchall()
#     conn.close()

#     if not results:
#         return "No revenue data found for the specified date range."

#     analysis = "Revenue Stream Percentages:\n"
#     for row in results:
#         analysis += f"\nRevenue Stream: {row[0]}"
#         analysis += f"\n  Total Inflow: {row[1]:.2f}"
#         analysis += f"\n  Percentage of Total: {row[2]:.2f}%\n"

#     return analysis

# # Add the analyze_expenses function to handle expense category queries
# def analyze_expenses(inputs: str) -> str:
#     """
#     Get comprehensive expense analysis including categories and patterns.
#     Accepts input as a JSON-like string containing 'period_start' and 'period_end'.
#     Args:
#         period_start (str): Start date in YYYY-MM-DD format
#         period_end (str): End date in YYYY-MM-DD format
#     Returns:
#         String containing detailed expense analysis
#     """
#     import json
#     try:
#         # Parse the input string into a dictionary
#         input_dict = json.loads(inputs)
#         period_start = input_dict["period_start"]
#         period_end = input_dict["period_end"]
#     except (ValueError, KeyError) as e:
#         return f"Invalid input format: {e}"

#     conn = sqlite3.connect("Bills.db")
#     cursor = conn.cursor()

#     query = """
#         WITH expense_analysis AS (
#             SELECT
#                 COALESCE(Expense_Category, 'Uncategorized') as category,
#                 COUNT(*) as transaction_count,
#                 SUM(Payment_Amount) as total_amount,
#                 AVG(Payment_Amount) as avg_amount,
#                 MIN(Payment_Amount) as min_amount,
#                 MAX(Payment_Amount) as max_amount
#             FROM Bill_Payments
#             WHERE Bill_Payment_Date BETWEEN ? AND ?
#             GROUP BY Expense_Category
#         )
#         SELECT
#             category,
#             transaction_count,
#             total_amount,
#             avg_amount,
#             min_amount,
#             max_amount,
#             (total_amount / (SELECT SUM(total_amount) FROM expense_analysis) * 100) as percentage
#         FROM expense_analysis
#         ORDER BY total_amount DESC
#     """

#     cursor.execute(query, (period_start, period_end))
#     results = cursor.fetchall()
#     conn.close()

#     analysis = "Comprehensive Expense Analysis:\n"
#     for row in results:
#         analysis += f"\nCategory: {row[0]}"
#         analysis += f"\n  Transaction Count: {row[1]}"
#         analysis += f"\n  Total Amount: {row[2]:.2f}"
#         analysis += f"\n  Average Amount: {row[3]:.2f}"
#         analysis += f"\n  Range: {row[4]:.2f} to {row[5]:.2f}"
#         analysis += f"\n  Percentage of Total: {row[6]:.2f}%"

#     return analysis

# # Function to parse dates from the query string using regex
# def parse_dates_from_query(query):
#     date_pattern = r"(january|february|march|april|may|june|july|august|september|october|november|december)\s*(\d{4})"
#     date_range_pattern = r"(\d{4})\s*to\s*(\d{4})"
#     year_pattern = r"(\d{4})"

#     # Matching a date range like "January 2023 to December 2023"
#     range_match = re.search(date_range_pattern, query.lower())
#     if range_match:
#         start_year, end_year = range_match.groups()
#         return f"{start_year}-01-01", f"{end_year}-12-31"

#     # Matching a single month-year (e.g., "January 2023")
#     match = re.search(date_pattern, query.lower())
#     if match:
#         month, year = match.groups()
#         month = datetime.strptime(month, "%B").month  # Convert month name to number

#         # Get the number of days in the month for the specified year
#         last_day = calendar.monthrange(int(year), month)[1]

#         return f"{year}-{month:02d}-01", f"{year}-{month:02d}-{last_day:02d}"

#     # Matching just the year (e.g., "2023")
#     year_match = re.search(year_pattern, query.lower())
#     if year_match:
#         year = year_match.group(1)
#         return f"{year}-01-01", f"{year}-12-31"

#     raise ValueError("Unable to parse dates from the query.")

# # LangChain setup for dynamic query handling
# llm = OpenAI(temperature=0)  # OpenAI LLM (can be customized for more complex queries)
# tools = [
#     Tool(
#         name="get_average_cash_flows",
#         func=get_average_cash_flows,
#         description="Calculate the average inflow and outflow for a given date range."
#     ),
#     Tool(
#         name="analyze_monthly_trends",
#         func=analyze_monthly_trends,
#         description="Analyze monthly cash flow trends for a specific year."
#     ),
#      Tool(
#         name="get_revenue_stream_percentages",
#         func=get_revenue_stream_percentages,
#         description="Calculate the percentage of cash inflows from each revenue stream for a given date range."
#     ),
#     Tool(
#         name="analyze_expenses",
#         func=analyze_expenses,
#         description="Get percentage of total cash outflows by major expense category for a given period."
#     ),
# ]

# # Initialize the agent
# agent = initialize_agent(
#     tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
# )

# def execute_query(query):
#     """
#     Execute the query by mapping it to the appropriate predefined function.
#     Uses LangChain for dynamic query handling.
#     """
#     try:
#         if "expense" in query.lower() and "category" in query.lower():
#             # Parse the date range from the query
#             start_date, end_date = parse_dates_from_query(query)
#             inputs = json.dumps({
#                 "period_start": start_date,
#                 "period_end": end_date
#             })
#             return analyze_expenses(inputs)

#         # Check if the query relates to revenue stream percentages
#         if "percentage" in query.lower() and "revenue stream" in query.lower():
#             start_date, end_date = parse_dates_from_query(query)
#             date_range = json.dumps({
#                 "start_date": start_date,
#                 "end_date": end_date
#             })
#             return get_revenue_stream_percentages(date_range)

#         # Check if the query asks for monthly trends (you can refine this regex as needed)
#         if "month" in query.lower() and "cash flow" in query.lower():
#             # Extract year from the query
#             year_match = re.search(r"\d{4}", query)
#             if year_match:
#                 year = year_match.group(0)
#                 return analyze_monthly_trends(year)

#         # Parse the dates from the query (for other types of queries)
#         start_date, end_date = parse_dates_from_query(query)

#         # Format the date range in JSON to pass to the agent
#         date_range = json.dumps({
#             "start_date": start_date,
#             "end_date": end_date
#         })

#         # Process the query using the LLM-based agent
#         response = agent.run(date_range)
#         return response
#     except Exception as e:
#         return f"Error processing query: {e}"


# # Interactive testing function
# def interactive_test():
#     print("Welcome to the Financial Analysis System!")
#     print("Type 'exit' to quit.")

#     while True:
#         user_query = input("Enter your question: ")
#         if user_query.lower() == "exit":
#             print("Goodbye!")
#             break
#         response = execute_query(user_query)
#         print(f"\nResponse:\n{response}\n")

# # Running the interactive test
# if __name__ == "__main__":
#     interactive_test()

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import sqlite3
import json
import re
from datetime import datetime
import calendar
from langchain.agents import initialize_agent, Tool, AgentType
from langchain.llms import OpenAI

from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
import sqlite3
import json
# FastAPI instance
app = FastAPI(title="Financial Analysis API")

# Database connection setup
db_path = "Bills.db"

def get_connection():
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row  # Return results as dictionaries
    return conn



# API Keys and Endpoint
ROOTIFY_API = os.getenv("ROOTIFY_API")
COMPANY_ID = os.getenv("COMPANY_ID")
if not ROOTIFY_API or not COMPANY_ID:
    raise ValueError("ROOTIFY_API and COMPANY_ID must be set as environment variables.")

# API Fetching Logic
def fetch_and_normalize_data():
    headers = {"Authorization": f"Bearer {ROOTIFY_API}"}

    # Fetch Invoice Payments
    invoice_url = f"https://api.example.com/companies/{COMPANY_ID}/invoice-payments"
    invoice_response = requests.get(invoice_url, headers=headers)
    if invoice_response.status_code != 200:
        raise HTTPException(status_code=500, detail="Failed to fetch invoice payments")
    invoice_data = pd.json_normalize(invoice_response.json())

    # Fetch Bill Payments
    bill_url = f"https://api.example.com/companies/{COMPANY_ID}/bill-payments"
    bill_response = requests.get(bill_url, headers=headers)
    if bill_response.status_code != 200:
        raise HTTPException(status_code=500, detail="Failed to fetch bill payments")
    bill_data = pd.json_normalize(bill_response.json())

    return invoice_data, bill_data

# Pydantic model for request validation
class DateRangeRequest(BaseModel):
    start_date: str
    end_date: str

class QueryRequest(BaseModel):
    query: str



# Function to get average cash flows
def get_average_cash_flows(inputs: str) -> str:
    try:
        input_dict = json.loads(inputs)
        start_date = input_dict["start_date"]
        end_date = input_dict["end_date"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    inflow_query = """
        SELECT AVG(Payment_Amount) as avg_inflow
        FROM Invoice_Payments
        WHERE Invoice_Payment_Date BETWEEN ? AND ?;
    """

    outflow_query = """
        SELECT AVG(Payment_Amount) as avg_outflow
        FROM Bill_Payments
        WHERE Bill_Payment_Date BETWEEN ? AND ?;
    """

    cursor.execute(inflow_query, (start_date, end_date))
    avg_inflow = cursor.fetchone()[0] or 0

    cursor.execute(outflow_query, (start_date, end_date))
    avg_outflow = cursor.fetchone()[0] or 0

    conn.close()

    return f"Average Inflow: {avg_inflow:.2f}, Average Outflow: {avg_outflow:.2f}"


# @tool("analyze_monthly_trends")
def analyze_monthly_trends(year: str) -> str:
    """
    Analyze monthly cash flow trends for a specific year.
    Args:
        year (str): Year to analyze
    Returns:
        String containing monthly trend analysis
    """
    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    monthly_query = """
        WITH monthly_inflows AS (
            SELECT
                strftime('%Y-%m', Invoice_Payment_Date) as month,
                SUM(Payment_Amount) as total_inflow
            FROM Invoice_Payments
            WHERE strftime('%Y', Invoice_Payment_Date) = ?
            GROUP BY month
        ),
        monthly_outflows AS (
            SELECT
                strftime('%Y-%m', Bill_Payment_Date) as month,
                SUM(Payment_Amount) as total_outflow
            FROM Bill_Payments
            WHERE strftime('%Y', Bill_Payment_Date) = ?
            GROUP BY month
        ),
        combined_data AS (
            SELECT
                mi.month as month,
                mi.total_inflow as total_inflow,
                mo.total_outflow as total_outflow
            FROM monthly_inflows mi
            LEFT JOIN monthly_outflows mo ON mi.month = mo.month
            UNION
            SELECT
                mo.month as month,
                mi.total_inflow as total_inflow,
                mo.total_outflow as total_outflow
            FROM monthly_outflows mo
            LEFT JOIN monthly_inflows mi ON mo.month = mi.month
        )
        SELECT
            month,
            COALESCE(total_inflow, 0) as total_inflow,
            COALESCE(total_outflow, 0) as total_outflow,
            COALESCE(total_inflow, 0) - COALESCE(total_outflow, 0) as net_flow
        FROM combined_data
        ORDER BY month;
    """

    cursor.execute(monthly_query, (year, year))
    results = cursor.fetchall()
    conn.close()

    if not results:
        return f"No data found for year {year}"

    # Finding months with highest and lowest net cash flow
    max_month = min_month = results[0]
    for row in results:
        if row[3] > max_month[3]: max_month = row
        if row[3] < min_month[3]: min_month = row

    analysis = f"Monthly Cash Flow Analysis for {year}:\n"
    for row in results:
        analysis += f"\nMonth {row[0]}:"
        analysis += f"\n  Inflow: {row[1]:.2f}"
        analysis += f"\n  Outflow: {row[2]:.2f}"
        analysis += f"\n  Net Flow: {row[3]:.2f}"

    analysis += f"\n\nThe month with the highest net cash flow is {max_month[0]} with a net flow of {max_month[3]:.2f}."
    analysis += f"\nThe month with the lowest net cash flow is {min_month[0]} with a net flow of {min_month[3]:.2f}."
    # For detailed insights about fluctuations, further query logic can be added.

    return analysis

# Function to calculate the percentage of inflows from each revenue stream
def get_revenue_stream_percentages(inputs: str) -> str:
    """
    Calculate the percentage of cash inflows from each revenue stream for a given date range.
    Args:
        inputs (str): JSON-like string containing 'start_date' and 'end_date'.
    Returns:
        String summarizing revenue stream percentages.
    """
    import json
    try:
        # Parse inputs
        input_dict = json.loads(inputs)
        start_date = input_dict["start_date"]
        end_date = input_dict["end_date"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    revenue_query = """
        WITH revenue_streams AS (
            SELECT
                Vendor_Contact_ID AS revenue_stream,
                SUM(Payment_Amount) AS total_inflow
            FROM Invoice_Payments
            WHERE Invoice_Payment_Date BETWEEN ? AND ?
            GROUP BY Vendor_Contact_ID
        )
        SELECT
            revenue_stream,
            total_inflow,
            (total_inflow * 1.0 / (SELECT SUM(total_inflow) FROM revenue_streams) * 100) AS percentage
        FROM revenue_streams
        ORDER BY total_inflow DESC
    """

    cursor.execute(revenue_query, (start_date, end_date))
    results = cursor.fetchall()
    conn.close()

    if not results:
        return "No revenue data found for the specified date range."

    analysis = "Revenue Stream Percentages:\n"
    for row in results:
        analysis += f"\nRevenue Stream: {row[0]}"
        analysis += f"\n  Total Inflow: {row[1]:.2f}"
        analysis += f"\n  Percentage of Total: {row[2]:.2f}%\n"

    return analysis

# Add the analyze_expenses function to handle expense category queries
def analyze_expenses(inputs: str) -> str:
    """
    Get comprehensive expense analysis including categories and patterns.
    Accepts input as a JSON-like string containing 'period_start' and 'period_end'.
    Args:
        period_start (str): Start date in YYYY-MM-DD format
        period_end (str): End date in YYYY-MM-DD format
    Returns:
        String containing detailed expense analysis
    """
    import json
    try:
        # Parse the input string into a dictionary
        input_dict = json.loads(inputs)
        period_start = input_dict["period_start"]
        period_end = input_dict["period_end"]
    except (ValueError, KeyError) as e:
        return f"Invalid input format: {e}"

    conn = sqlite3.connect("Bills.db")
    cursor = conn.cursor()

    query = """
        WITH expense_analysis AS (
            SELECT
                COALESCE(Expense_Category, 'Uncategorized') as category,
                COUNT(*) as transaction_count,
                SUM(Payment_Amount) as total_amount,
                AVG(Payment_Amount) as avg_amount,
                MIN(Payment_Amount) as min_amount,
                MAX(Payment_Amount) as max_amount
            FROM Bill_Payments
            WHERE Bill_Payment_Date BETWEEN ? AND ?
            GROUP BY Expense_Category
        )
        SELECT
            category,
            transaction_count,
            total_amount,
            avg_amount,
            min_amount,
            max_amount,
            (total_amount / (SELECT SUM(total_amount) FROM expense_analysis) * 100) as percentage
        FROM expense_analysis
        ORDER BY total_amount DESC
    """

    cursor.execute(query, (period_start, period_end))
    results = cursor.fetchall()
    conn.close()

    analysis = "Comprehensive Expense Analysis:\n"
    for row in results:
        analysis += f"\nCategory: {row[0]}"
        analysis += f"\n  Transaction Count: {row[1]}"
        analysis += f"\n  Total Amount: {row[2]:.2f}"
        analysis += f"\n  Average Amount: {row[3]:.2f}"
        analysis += f"\n  Range: {row[4]:.2f} to {row[5]:.2f}"
        analysis += f"\n  Percentage of Total: {row[6]:.2f}%"

    return analysis

# Function to parse dates from the query string using regex
def parse_dates_from_query(query):
    date_pattern = r"(january|february|march|april|may|june|july|august|september|october|november|december)\s*(\d{4})"
    date_range_pattern = r"(\d{4})\s*to\s*(\d{4})"
    year_pattern = r"(\d{4})"

    # Matching a date range like "January 2023 to December 2023"
    range_match = re.search(date_range_pattern, query.lower())
    if range_match:
        start_year, end_year = range_match.groups()
        return f"{start_year}-01-01", f"{end_year}-12-31"

    # Matching a single month-year (e.g., "January 2023")
    match = re.search(date_pattern, query.lower())
    if match:
        month, year = match.groups()
        month = datetime.strptime(month, "%B").month  # Convert month name to number

        # Get the number of days in the month for the specified year
        last_day = calendar.monthrange(int(year), month)[1]

        return f"{year}-{month:02d}-01", f"{year}-{month:02d}-{last_day:02d}"

    # Matching just the year (e.g., "2023")
    year_match = re.search(year_pattern, query.lower())
    if year_match:
        year = year_match.group(1)
        return f"{year}-01-01", f"{year}-12-31"

    raise ValueError("Unable to parse dates from the query.")

# LangChain setup for dynamic query handling
llm = OpenAI(temperature=0)  # OpenAI LLM (can be customized for more complex queries)
tools = [
    Tool(
        name="get_average_cash_flows",
        func=get_average_cash_flows,
        description="Calculate the average inflow and outflow for a given date range."
    ),
    Tool(
        name="analyze_monthly_trends",
        func=analyze_monthly_trends,
        description="Analyze monthly cash flow trends for a specific year."
    ),
     Tool(
        name="get_revenue_stream_percentages",
        func=get_revenue_stream_percentages,
        description="Calculate the percentage of cash inflows from each revenue stream for a given date range."
    ),
    Tool(
        name="analyze_expenses",
        func=analyze_expenses,
        description="Get percentage of total cash outflows by major expense category for a given period."
    ),
]

# Initialize the agent
agent = initialize_agent(
    tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)

def execute_query(query):
    """
    Execute the query by mapping it to the appropriate predefined function.
    Uses LangChain for dynamic query handling.
    """
    try:
        if "expense" in query.lower() and "category" in query.lower():
            # Parse the date range from the query
            start_date, end_date = parse_dates_from_query(query)
            inputs = json.dumps({
                "period_start": start_date,
                "period_end": end_date
            })
            return analyze_expenses(inputs)

        # Check if the query relates to revenue stream percentages
        if "percentage" in query.lower() and "revenue stream" in query.lower():
            start_date, end_date = parse_dates_from_query(query)
            date_range = json.dumps({
                "start_date": start_date,
                "end_date": end_date
            })
            return get_revenue_stream_percentages(date_range)

        # Check if the query asks for monthly trends (you can refine this regex as needed)
        if "month" in query.lower() and "cash flow" in query.lower():
            # Extract year from the query
            year_match = re.search(r"\d{4}", query)
            if year_match:
                year = year_match.group(0)
                return analyze_monthly_trends(year)

        # Parse the dates from the query (for other types of queries)
        start_date, end_date = parse_dates_from_query(query)

        # Format the date range in JSON to pass to the agent
        date_range = json.dumps({
            "start_date": start_date,
            "end_date": end_date
        })

        # Process the query using the LLM-based agent
        response = agent.run(date_range)
        return response
    except Exception as e:
        return f"Error processing query: {e}"


# # Interactive testing function
# def interactive_test():
#     print("Welcome to the Financial Analysis System!")
#     print("Type 'exit' to quit.")

#     while True:
#         user_query = input("Enter your question: ")
#         if user_query.lower() == "exit":
#             print("Goodbye!")
#             break
#         response = execute_query(user_query)
#         print(f"\nResponse:\n{response}\n")


@app.get("/")
def root():
    return {"message": "Welcome to the Financial Analysis API!"}



@app.post("/fetch-data")
def api_fetch_data():
    try:
        invoice_data, bill_data = fetch_and_normalize_data()
        return {
            "invoice_payments": invoice_data.to_dict(orient="records"),
            "bill_payments": bill_data.to_dict(orient="records")
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



# API endpoints for specific functions
@app.post("/average-cash-flows")
def api_get_average_cash_flows(request: DateRangeRequest):
    inputs = request.dict()
    response = get_average_cash_flows(inputs)
    return {"result": response}

@app.post("/monthly-trends")
def api_analyze_monthly_trends(year: str):
    response = analyze_monthly_trends(year)
    return {"result": response}

@app.post("/revenue-stream-percentages")
def api_get_revenue_stream_percentages(request: DateRangeRequest):
    inputs = request.dict()
    response = get_revenue_stream_percentages(inputs)
    return {"result": response}

@app.post("/expense-analysis")
def api_analyze_expenses(request: DateRangeRequest):
    inputs = request.dict()
    response = analyze_expenses(inputs)
    return {"result": response}

@app.post("/query")
def api_execute_query(request: QueryRequest):
    response = execute_query(request.query)
    return {"result": response}



# Import FastAPI and Uvicorn
from fastapi import FastAPI
from fastapi.responses import JSONResponse
import uvicorn

# Initialize the FastAPI app
app = FastAPI()



# # Step 5: Insert Data from DataFrame to SQLite Table
# # Using 'if_exists="replace"' to overwrite any existing table
# df.to_sql("Bill_Payments", conn, if_exists="append", index=False)
# df_2.to_sql("Bill_Credit_Notes", conn, if_exists="append", index=False)
# df_a.to_sql("Invoice_Payments", conn, if_exists="append", index=False)
# df_b.to_sql("Invoice_Credit_Notes", conn, if_exists="append", index=False)
# df_c.to_sql("Invoices", conn, if_exists="append", index=False)



# Define global DataFrames for use in API endpoints
global_df = df  # Final DataFrame for Bill_Payments
global_df_2 = df_2  # Final DataFrame for Bill_Credit_Notes
global_df_a = df_a  # Final DataFrame for Invoice_Payments
global_df_b = df_b  # Final DataFrame for Invoice_Credit_Notes
global_df_c = df_c  # Final DataFrame for Invoices


@app.get("/")
def home():
    return {"message": "API is running"}

@app.get("/get-bill-payments")
def get_bill_payments():
    return JSONResponse(content=global_df.to_dict(orient="records"))

@app.get("/get-bill-credit-notes")
def get_bill_credit_notes():
    return JSONResponse(content=global_df_2.to_dict(orient="records"))

@app.get("/get-invoice-payments")
def get_invoice_payments():
    return JSONResponse(content=global_df_a.to_dict(orient="records"))

@app.get("/get-invoice-credit-notes")
def get_invoice_credit_notes():
    return JSONResponse(content=global_df_b.to_dict(orient="records"))

@app.get("/get-invoices")
def get_invoices():
    return JSONResponse(content=global_df_c.to_dict(orient="records"))

# To run the app, uncomment the following line and run the cell:
# uvicorn.run(app, host="0.0.0.0", port=8000)

